#!/usr/bin/env python3
"""
éªŒè¯tokenizerå’Œembeddingæƒé‡çš„å¯¹åº”å…³ç³»
"""
import torch
from transformers import AutoTokenizer

MODEL_PATH = "/home/aoi/new/resume/Dev_container/hunyuan_model/qwen2.5-0.5b-instruct"
WEIGHT_PATH = "/home/aoi/new/resume/Dev_container/hunyuan_model/qwen2.5-0.5b-instruct/qwen2.5-0.5b-instruct.pt"

def main():
    print("=" * 60)
    print("ã€éªŒè¯Tokenizerä¸Embeddingæƒé‡çš„å¯¹åº”å…³ç³»ã€‘")
    print("=" * 60)
    
    # 1. åŠ è½½Qwen tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    print(f"\nâœ… åŠ è½½Qwen tokenizer")
    print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"  æ¨¡å‹æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
    
    # 2. åŠ è½½embeddingæƒé‡
    state_dict = torch.load(WEIGHT_PATH, map_location="cpu")
    embed_weight = state_dict["model.embed_tokens.weight"]
    print(f"\nâœ… åŠ è½½Embeddingæƒé‡")
    print(f"  æƒé‡å½¢çŠ¶: {embed_weight.shape}")
    print(f"  vocab_size: {embed_weight.shape[0]}")
    print(f"  d_model: {embed_weight.shape[1]}")
    
    # 3. éªŒè¯å¯¹åº”å…³ç³»
    if tokenizer.vocab_size == embed_weight.shape[0]:
        print(f"\nâœ… éªŒè¯é€šè¿‡: tokenizerè¯æ±‡è¡¨å¤§å°ä¸embeddingæƒé‡å®Œå…¨åŒ¹é…!")
    else:
        print(f"\nâŒ è­¦å‘Š: è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…!")
        print(f"  tokenizer: {tokenizer.vocab_size}")
        print(f"  embedding: {embed_weight.shape[0]}")
    
    # 4. æµ‹è¯•å®é™…åˆ†è¯å’Œembedding
    test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
    token_ids = tokenizer.encode(test_text, add_special_tokens=False)
    print(f"\nã€æµ‹è¯•ç¤ºä¾‹ã€‘")
    print(f"  æ–‡æœ¬: {test_text}")
    print(f"  Token IDs: {token_ids}")
    
    # æ£€æŸ¥token IDæ˜¯å¦åœ¨åˆæ³•èŒƒå›´å†…
    for tid in token_ids:
        if tid >= embed_weight.shape[0]:
            print(f"  âŒ é”™è¯¯: Token ID {tid} è¶…å‡ºembeddingèŒƒå›´ [0, {embed_weight.shape[0]-1}]")
        else:
            # è·å–å¯¹åº”çš„embeddingå‘é‡
            embed_vec = embed_weight[tid]
            print(f"  âœ… Token ID {tid} -> Embeddingå‘é‡ {embed_vec.shape}, å‰3ä¸ªå€¼: {embed_vec[:3].tolist()}")
    
    # 5. å¯¹æ¯”ä¸åŒtokenizerçš„å·®å¼‚
    print(f"\n" + "=" * 60)
    print("ã€å¯¹æ¯”: å¦‚æœç”¨é”™è¯¯çš„tokenizerä¼šæ€æ ·ã€‘")
    print("=" * 60)
    
    try:
        # å°è¯•åŠ è½½ä¸€ä¸ªä¸åŒçš„tokenizer (æ¯”å¦‚GPT2)
        from transformers import GPT2Tokenizer
        gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        gpt2_token_ids = gpt2_tokenizer.encode(test_text)
        
        print(f"\nå¦‚æœç”¨GPT2 tokenizer:")
        print(f"  GPT2è¯æ±‡è¡¨å¤§å°: {gpt2_tokenizer.vocab_size}")
        print(f"  ç›¸åŒæ–‡æœ¬ '{test_text}' çš„Token IDs: {gpt2_token_ids}")
        print(f"  å¯¹æ¯”Qwen Token IDs: {token_ids}")
        print(f"\nâš ï¸ çœ‹åˆ°äº†å—? å®Œå…¨ä¸åŒçš„Token IDs!")
        print(f"  å¦‚æœç”¨GPT2çš„token IDså»æŸ¥Qwençš„embeddingï¼Œä¼šå¾—åˆ°é”™è¯¯çš„è¯å‘é‡!")
        
    except Exception as e:
        print(f"  (æ— æ³•åŠ è½½GPT2 tokenizerç”¨äºå¯¹æ¯”: {e})")
    
    # 6. ç»“è®º
    print(f"\n" + "=" * 60)
    print("ã€é‡è¦ç»“è®ºã€‘")
    print("=" * 60)
    print("âœ… æˆ‘ä»¬çš„å®ç°æ˜¯æ­£ç¡®çš„:")
    print("  1. ä½¿ç”¨Qwenå®˜æ–¹tokenizerç”Ÿæˆtoken IDs")
    print("  2. Token IDsåŒ¹é…Qwenæ¨¡å‹çš„embeddingæƒé‡")
    print("  3. æ¯ä¸ªtoken IDéƒ½èƒ½æ­£ç¡®æ˜ å°„åˆ°å¯¹åº”çš„è¯å‘é‡")
    print("\nâŒ å¦‚æœç”¨é”™tokenizerä¼šå¯¼è‡´:")
    print("  1. Token IDså®Œå…¨ä¸åŒ")
    print("  2. Embeddingå±‚è¿”å›é”™è¯¯çš„è¯å‘é‡")
    print("  3. æ¨¡å‹è¾“å‡ºå®Œå…¨æ˜¯ä¹±ç ")
    print("\nğŸ’¡ è®°ä½: æ¨¡å‹å’Œtokenizerå¿…é¡»é…å¥—ä½¿ç”¨!")

if __name__ == "__main__":
    main()
