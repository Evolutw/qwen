#!/usr/bin/env python3
"""
ç›´è§‚å¯¹æ¯”ï¼šæ­£ç¡®tokenizer vs é”™è¯¯tokenizerçš„å½±å“
"""
import torch
from transformers import AutoTokenizer

MODEL_PATH = "/home/aoi/new/resume/Dev_container/hunyuan_model/qwen2.5-0.5b-instruct"
WEIGHT_PATH = "/home/aoi/new/resume/Dev_container/hunyuan_model/qwen2.5-0.5b-instruct/qwen2.5-0.5b-instruct.pt"

def visualize_tokenization():
    print("=" * 70)
    print("         ã€ç›´è§‚å¯¹æ¯”ï¼šä¸åŒTokenizeräº§ç”Ÿçš„Token IDsã€‘")
    print("=" * 70)
    
    # åŠ è½½Qwen tokenizerå’Œembedding
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    state_dict = torch.load(WEIGHT_PATH, map_location="cpu", weights_only=False)
    embed_weight = state_dict["model.embed_tokens.weight"]
    
    test_texts = [
        "ä½ å¥½",
        "äººå·¥æ™ºèƒ½",
        "æ·±åº¦å­¦ä¹ ",
        "Hello World"
    ]
    
    for text in test_texts:
        print(f"\nğŸ“ åŸå§‹æ–‡æœ¬: '{text}'")
        print("-" * 70)
        
        # Qwen tokenizer (æ­£ç¡®çš„)
        qwen_ids = tokenizer.encode(text, add_special_tokens=False)
        print(f"âœ… Qwen Tokenizer â†’ Token IDs: {qwen_ids}")
        print(f"   Tokenæ•°é‡: {len(qwen_ids)}")
        
        # æ˜¾ç¤ºæ¯ä¸ªtokenå¯¹åº”çš„embeddingå‘é‡èŒƒæ•°ï¼ˆè¡¨ç¤ºå‘é‡å¤§å°ï¼‰
        qwen_norms = []
        for tid in qwen_ids:
            if tid < embed_weight.shape[0]:
                vec = embed_weight[tid]
                norm = torch.norm(vec).item()
                qwen_norms.append(f"{norm:.3f}")
        print(f"   å¯¹åº”Embeddingå‘é‡èŒƒæ•°: [{', '.join(qwen_norms)}]")
        
        # å°è¯•ç”¨å‡è®¾çš„"é”™è¯¯"token IDs
        # æ¨¡æ‹Ÿå¦‚æœç”¨äº†å…¶ä»–æ¨¡å‹çš„tokenizerä¼šæ€æ ·
        wrong_ids = [i * 1000 % embed_weight.shape[0] for i in range(1, len(qwen_ids) + 1)]
        print(f"\nâŒ å‡è®¾ç”¨äº†é”™è¯¯çš„Tokenizer â†’ Token IDs: {wrong_ids}")
        print(f"   (è¿™äº›IDåœ¨Qwençš„embeddingä¸­æŒ‡å‘å®Œå…¨ä¸ç›¸å…³çš„è¯)")
        
        wrong_norms = []
        for tid in wrong_ids:
            vec = embed_weight[tid]
            norm = torch.norm(vec).item()
            wrong_norms.append(f"{norm:.3f}")
        print(f"   é”™è¯¯æ˜ å°„çš„Embeddingå‘é‡èŒƒæ•°: [{', '.join(wrong_norms)}]")
        
        print(f"\nğŸ’¡ ç»“è®º: å³ä½¿Tokenæ•°é‡ç›¸åŒï¼Œä½†IDå®Œå…¨ä¸åŒï¼ŒæŒ‡å‘çš„è¯å‘é‡ä¹Ÿå®Œå…¨ä¸åŒ!")
    
    print("\n" + "=" * 70)
    print("ã€å…³é”®è¦ç‚¹æ€»ç»“ã€‘")
    print("=" * 70)
    print("""
1ï¸âƒ£  æ¯ä¸ªæ¨¡å‹çš„Tokenizeréƒ½æ˜¯å”¯ä¸€çš„
    - Qwenæœ‰è‡ªå·±çš„è¯æ±‡è¡¨ï¼ˆ151936ä¸ªtokenï¼‰
    - GPTã€LLaMAç­‰éƒ½æœ‰å„è‡ªä¸åŒçš„è¯æ±‡è¡¨

2ï¸âƒ£  Token IDæ˜¯è¯æ±‡è¡¨çš„ç´¢å¼•
    - "ä½ å¥½" åœ¨Qwenä¸­å¯èƒ½æ˜¯ [108386, 3837]
    - åœ¨GPTä¸­å¯èƒ½å®Œå…¨ä¸åŒï¼Œå¦‚ [19526, 254, 25001, ...]

3ï¸âƒ£  Embeddingæƒé‡ä¸Tokenizerä¸¥æ ¼å¯¹åº”
    - EmbeddingçŸ©é˜µç¬¬iè¡Œ = è¯æ±‡è¡¨ç¬¬iä¸ªtokençš„å‘é‡è¡¨ç¤º
    - ç”¨é”™tokenizer â†’ æ‹¿åˆ°é”™è¯¯çš„ID â†’ æŸ¥åˆ°é”™è¯¯çš„å‘é‡ â†’ è¾“å‡ºä¹±ç 

4ï¸âƒ£  æˆ‘ä»¬çš„å®ç°æ˜¯æ­£ç¡®çš„
    âœ… qwen_tokenize.py ä½¿ç”¨ AutoTokenizer.from_pretrained(Qwenæ¨¡å‹)
    âœ… C++è°ƒç”¨è¿™ä¸ªPythonè„šæœ¬ç”Ÿæˆtoken IDs
    âœ… Token IDsæ­£ç¡®æ˜ å°„åˆ°Qwençš„embeddingæƒé‡
    âœ… æ•´ä¸ªæµç¨‹ä¿è¯äº†ä¸€è‡´æ€§

âš ï¸  åƒä¸‡ä¸è¦æ··ç”¨ä¸åŒæ¨¡å‹çš„tokenizerå’Œæƒé‡!
    """)

if __name__ == "__main__":
    visualize_tokenization()
