import os
import json
import argparse
import torch
from safetensors.torch import load_file

def parse_args():
    parser = argparse.ArgumentParser(description="Convert Qwen safetensors to .pt")
    parser.add_argument("--input", dest="input_path", default=os.environ.get("QWEN_SAFETENSORS_PATH", ""))
    parser.add_argument("--output", dest="output_path", default=os.environ.get("QWEN_WEIGHT_PATH", ""))
    parser.add_argument("--model-dir", dest="model_dir", default=os.environ.get("QWEN_MODEL_DIR", ""))
    return parser.parse_args()


args = parse_args()

# ===================== è·¯å¾„é…ç½®ï¼ˆä¼˜å…ˆå‘½ä»¤è¡Œï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼‰ =====================
if args.model_dir and not args.input_path:
    args.input_path = os.path.join(args.model_dir, "model.safetensors")
if args.model_dir and not args.output_path:
    args.output_path = os.path.join(args.model_dir, "qwen2.5-0.5b-instruct.pt")

SAFETENSORS_INPUT_PATH = args.input_path
PT_OUTPUT_PATH = args.output_path

if not SAFETENSORS_INPUT_PATH or not PT_OUTPUT_PATH:
    raise SystemExit("Please set --input/--output or QWEN_MODEL_DIR/QWEN_SAFETENSORS_PATH/QWEN_WEIGHT_PATH.")

# ===================== æ‰‹åŠ¨æŒ‡å®šå…³é”®æƒé‡åï¼ˆé€‚é…ä½ çš„æƒé‡ï¼‰ =====================
EMBEDDING_KEY = "model.embed_tokens.weight"  # Embeddingå±‚æƒé‡
LMHEAD_KEY = "model.embed_tokens.weight"     # LMHeadä¸Embeddingå…±äº«æƒé‡ï¼Œæ— éœ€ä¿®æ”¹
LMHEAD_INDEPENDENT = False                   # æ ‡è®°ï¼šæ— ç‹¬ç«‹LMHeadæƒé‡

# ===================== è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½• =====================
output_dir = os.path.dirname(PT_OUTPUT_PATH)
os.makedirs(output_dir, exist_ok=True)
print(f"âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡ï¼š{output_dir}")

# ===================== åŠ è½½safetensorsæƒé‡ï¼ˆæ”¯æŒåˆ†ç‰‡ï¼‰ =====================
print(f"\n[Step 1/4] æ­£åœ¨åŠ è½½safetensorsæ–‡ä»¶ï¼š\n{SAFETENSORS_INPUT_PATH}")
try:
    state_dict = {}
    if SAFETENSORS_INPUT_PATH.endswith(".index.json"):
        index_path = SAFETENSORS_INPUT_PATH
    else:
        index_path = os.path.join(os.path.dirname(SAFETENSORS_INPUT_PATH), "model.safetensors.index.json")

    if os.path.isfile(index_path):
        with open(index_path, "r", encoding="utf-8") as f:
            index = json.load(f)
        weight_map = index.get("weight_map", {})
        shard_files = sorted(set(weight_map.values()))
        if not shard_files:
            raise RuntimeError("weight_map is empty in index.json")
        base_dir = os.path.dirname(index_path)
        for shard in shard_files:
            shard_path = os.path.join(base_dir, shard)
            print(f"  loading shard: {shard}")
            state_dict.update(load_file(shard_path, device="cpu"))
    else:
        state_dict = load_file(SAFETENSORS_INPUT_PATH, device="cpu")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(state_dict)} ä¸ªæƒé‡å¼ é‡")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥ï¼š{e}")
    print("æç¤ºï¼šè¯·ç¡®è®¤safetensorsæ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œä¸”æ–‡ä»¶æœªæŸå")
    exit(1)

# ===================== éªŒè¯å…³é”®æƒé‡ =====================
print(f"\n[Step 2/4] éªŒè¯Qwenå…³é”®æƒé‡")
valid_flag = True
vocab_size = None
d_model = None

# éªŒè¯Embeddingæƒé‡
if EMBEDDING_KEY in state_dict:
    tensor_shape = state_dict[EMBEDDING_KEY].shape
    tensor_dtype = state_dict[EMBEDDING_KEY].dtype
    print(f"âœ… {EMBEDDING_KEY}ï¼šå½¢çŠ¶={tensor_shape}ï¼Œæ•°æ®ç±»å‹={tensor_dtype}")
    # æå–æ¨¡å‹å‚æ•°ï¼ˆEmbeddingæƒé‡å½¢çŠ¶ï¼š[vocab_size, d_model]ï¼‰
    vocab_size, d_model = tensor_shape
    print(f"   â†’ æå–æ¨¡å‹å‚æ•°ï¼švocab_size={vocab_size}ï¼Œd_model={d_model}")
else:
    print(f"âŒ {EMBEDDING_KEY} ä¸å­˜åœ¨äºæƒé‡ä¸­ï¼")
    valid_flag = False

# éªŒè¯LMHeadæƒé‡ï¼ˆå…±äº«æƒé‡æ— éœ€é¢å¤–æ£€æŸ¥ï¼‰
print(f"â„¹ï¸ LMHeadä¸Embeddingå±‚å…±äº«æƒé‡ï¼š{LMHEAD_KEY}ï¼ˆæ— ç‹¬ç«‹LMHeadæƒé‡ï¼‰")

if not valid_flag:
    print("\næç¤ºï¼šè¯·ç¡®è®¤æƒé‡æ–‡ä»¶ä¸ºQwen2.5ç³»åˆ—æ¨¡å‹")
    exit(1)

# ===================== ä¿å­˜ä¸ºLibTorchå…¼å®¹çš„.ptæ ¼å¼ =====================
print(f"\n[Step 3/4] æ­£åœ¨ä¿å­˜.ptæƒé‡æ–‡ä»¶ï¼š\n{PT_OUTPUT_PATH}")
try:
    torch.save(state_dict, PT_OUTPUT_PATH)
    file_size = os.path.getsize(PT_OUTPUT_PATH) / 1024 / 1024 / 1024
    print(f"âœ… æƒé‡ä¿å­˜æˆåŠŸï¼Œæ–‡ä»¶å¤§å°ï¼š{file_size:.2f} GB")
except Exception as e:
    print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{e}")
    print("æç¤ºï¼šè¯·ç¡®è®¤è¾“å‡ºç›®å½•æœ‰å†™å…¥æƒé™ï¼ˆå¯å°è¯•sudoè¿è¡Œè„šæœ¬ï¼‰")
    exit(1)

# ===================== éªŒè¯.ptæ–‡ä»¶æœ‰æ•ˆæ€§ =====================
print(f"\n[Step 4/4] éªŒè¯.ptæ–‡ä»¶æ˜¯å¦å¯æ­£å¸¸åŠ è½½")
try:
    loaded_state_dict = torch.load(PT_OUTPUT_PATH, map_location="cpu")
    # å¯¹æ¯”Embeddingæƒé‡æ˜¯å¦ä¸€è‡´
    embed_safetensors = state_dict[EMBEDDING_KEY]
    embed_pt = loaded_state_dict[EMBEDDING_KEY]
    tensor_diff = torch.max(torch.abs(embed_safetensors - embed_pt)).item()
    
    if tensor_diff < 1e-6:
        print(f"âœ… .ptæ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œæƒé‡æœ€å¤§å·®å€¼ï¼š{tensor_diff:.6f}")
    else:
        print(f"âŒ .ptæ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œæƒé‡æœ€å¤§å·®å€¼ï¼š{tensor_diff:.6f}")
        exit(1)
except Exception as e:
    print(f"âŒ éªŒè¯å¤±è´¥ï¼š{e}")
    exit(1)

# ===================== è¾“å‡ºæœ€ç»ˆæç¤º =====================
print(f"\nğŸ‰ æƒé‡è½¬æ¢å…¨éƒ¨å®Œæˆï¼")
print(f"ğŸ“Œ åŸå§‹safetensorsï¼š{SAFETENSORS_INPUT_PATH}")
print(f"ğŸ“Œ è½¬æ¢å.ptæ–‡ä»¶ï¼š{PT_OUTPUT_PATH}")
print(f"ğŸ“Œ æ¨¡å‹å…³é”®å‚æ•°ï¼ˆåç»­C++éœ€ä½¿ç”¨ï¼‰ï¼š")
print(f"   - vocab_size={vocab_size}")
print(f"   - d_model={d_model}")
print(f"   - Embeddingæƒé‡åï¼š{EMBEDDING_KEY}")
print(f"   - LMHeadæƒé‡ï¼šä¸Embeddingå…±äº«ï¼ˆ{LMHEAD_KEY}ï¼‰ï¼Œæ— ç‹¬ç«‹æƒé‡")
print(f"   - æ¨¡å‹å±‚æ•°ï¼š24å±‚ï¼ˆä»æƒé‡é”®å`model.layers.23`æ¨æ–­ï¼Œåç»­C++éœ€è®¾ç½®num_layers=24ï¼‰")
print(f"   - æ³¨æ„åŠ›å¤´æ•°ï¼š8ï¼ˆQwen2.5ç³»åˆ—é»˜è®¤ï¼Œå¯é€šè¿‡d_model/head_dim=64éªŒè¯ï¼Œhead_dim=64ï¼‰")

