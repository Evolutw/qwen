#include "../include/qwen_model.h"
#include "../include/qwen_env.h"
#include "../include/qwen_model_config.h"
#include <iostream>
#include <vector>
#include <chrono>
#include <algorithm>
#include <random>

// é…ç½®Qwenæ¨¡å‹å‚æ•°ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
const auto& QWEN_CFG = qwen::get_model_config();
const std::string WEIGHT_PATH = qwen::get_weight_path();
const std::string TOKENIZER_SCRIPT = qwen::get_tokenizer_script();
const std::string TOKENIZER_MODEL_DIR = qwen::get_tokenizer_model_dir();
const std::string PYTHON_CMD = qwen::get_python_cmd();

// ç¼–ç æ–‡æœ¬ä¸ºtoken IDs
std::vector<int64_t> encode_text(const std::string& text) {
    std::string cmd = PYTHON_CMD + " " + TOKENIZER_SCRIPT + " \"" + text + "\" 2>/dev/null";
    
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) {
        throw std::runtime_error("æ— æ³•æ‰§è¡Œåˆ†è¯å‘½ä»¤");
    }
    
    char buffer[4096];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    pclose(pipe);
    
    // è§£æJSONæ ¼å¼çš„token IDs
    std::vector<int64_t> token_ids;
    size_t start = result.find("[");
    size_t end = result.find("]");
    if (start != std::string::npos && end != std::string::npos) {
        std::string ids_str = result.substr(start + 1, end - start - 1);
        std::stringstream ss(ids_str);
        std::string item;
        while (std::getline(ss, item, ',')) {
            token_ids.push_back(std::stoll(item));
        }
    }
    
    return token_ids;
}

// è§£ç token IDsä¸ºæ–‡æœ¬
std::string decode_tokens(const std::vector<int64_t>& token_ids) {
    if (token_ids.empty()) return "[ç©º]";
    
    std::string ids_str = "[";
    for (size_t i = 0; i < token_ids.size(); ++i) {
        ids_str += std::to_string(token_ids[i]);
        if (i < token_ids.size() - 1) ids_str += ",";
    }
    ids_str += "]";
    
    std::string cmd = PYTHON_CMD + " -c \"from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('" + TOKENIZER_MODEL_DIR + "', trust_remote_code=True); print(tokenizer.decode(" + ids_str + "), end='')\" 2>/dev/null";
    
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) return "[è§£ç å¤±è´¥]";
    
    char buffer[4096];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    pclose(pipe);
    
    return result.empty() ? "[ç©º]" : result;
}

// Temperatureé‡‡æ ·ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
int64_t sample_with_temperature(const torch::Tensor& logits, float temperature = 1.0, int top_k = 50) {
    torch::Tensor probs;
    
    if (temperature <= 0.0001) {
        // temperatureæ¥è¿‘0ï¼Œä½¿ç”¨è´ªå©ªé‡‡æ ·
        return logits.argmax(-1).item<int64_t>();
    }
    
    // åº”ç”¨temperature
    torch::Tensor scaled_logits = logits / temperature;
    
    // Top-ké‡‡æ ·
    if (top_k > 0 && top_k < logits.size(-1)) {
        auto topk_result = torch::topk(scaled_logits, top_k, -1);
        auto topk_values = std::get<0>(topk_result);
        auto topk_indices = std::get<1>(topk_result);
        
        // è®¡ç®—æ¦‚ç‡
        probs = torch::softmax(topk_values, -1);
        
        // é‡‡æ ·
        torch::Tensor cumsum = torch::cumsum(probs, -1);
        float random_val = static_cast<float>(rand()) / RAND_MAX;
        
        for (int i = 0; i < top_k; ++i) {
            if (cumsum[i].item<float>() >= random_val) {
                return topk_indices[i].item<int64_t>();
            }
        }
        return topk_indices[-1].item<int64_t>();
    } else {
        // æ ‡å‡†softmaxé‡‡æ ·
        probs = torch::softmax(scaled_logits, -1);
        torch::Tensor cumsum = torch::cumsum(probs, -1);
        float random_val = static_cast<float>(rand()) / RAND_MAX;
        
        for (int i = 0; i < probs.size(-1); ++i) {
            if (cumsum[i].item<float>() >= random_val) {
                return i;
            }
        }
        return probs.size(-1) - 1;
    }
}

// æ”¹è¿›çš„ç”Ÿæˆå‡½æ•°
std::vector<int64_t> generate_text(
    QwenModel& model,
    const torch::Tensor& input_ids,
    int max_new_tokens = 50,
    float temperature = 0.7,
    int top_k = 50,
    int64_t eos_token_id = -1,
    bool verbose = true
) {
    if (eos_token_id < 0) {
        eos_token_id = QWEN_CFG.eos_token_id;
    }
    auto device = input_ids.device();
    std::vector<int64_t> generated_tokens;
    
    // å¤åˆ¶è¾“å…¥tokensï¼ˆå…ˆç§»åˆ°CPUå†è®¿é—®ï¼‰
    auto input_cpu = input_ids.to(torch::kCPU);
    auto input_accessor = input_cpu.accessor<int64_t, 2>();
    for (int i = 0; i < input_cpu.size(1); ++i) {
        generated_tokens.push_back(input_accessor[0][i]);
    }
    
    // æ¸…ç©ºKVç¼“å­˜
    model->clear_cache();
    
    torch::NoGradGuard no_grad;
    
    // Prefillé˜¶æ®µï¼šå¤„ç†æ‰€æœ‰è¾“å…¥tokens
    if (verbose) std::cout << "Prefill... " << std::flush;
    auto start_prefill = std::chrono::high_resolution_clock::now();
    torch::Tensor logits = model->forward(input_ids, true);
    auto end_prefill = std::chrono::high_resolution_clock::now();
    if (verbose) {
        auto prefill_time = std::chrono::duration_cast<std::chrono::milliseconds>(end_prefill - start_prefill).count();
        std::cout << "(" << prefill_time << "ms) " << std::flush;
    }
    
    // è·å–æœ€åä¸€ä¸ªä½ç½®çš„logitså¹¶é‡‡æ ·
    torch::Tensor next_token_logits = logits[0][-1];
    int64_t next_token = sample_with_temperature(next_token_logits, temperature, top_k);
    generated_tokens.push_back(next_token);
    
    if (verbose) std::cout << "\nç”Ÿæˆä¸­: " << std::flush;
    
    // è‡ªå›å½’ç”Ÿæˆ
    auto start_decode = std::chrono::high_resolution_clock::now();
    for (int i = 1; i < max_new_tokens; ++i) {
        // æ£€æŸ¥æ˜¯å¦é‡åˆ°EOS
        if (next_token == eos_token_id) {
            if (verbose) std::cout << " [EOS]" << std::endl;
            break;
        }
        
        if (verbose && i % 5 == 0) std::cout << "." << std::flush;
        
        // å°†æ–°tokenä½œä¸ºè¾“å…¥ï¼ˆä½¿ç”¨KVç¼“å­˜ï¼‰
        torch::Tensor next_input = torch::tensor({{next_token}}, torch::kInt64).to(device);
        logits = model->forward(next_input, true);
        
        // é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token_logits = logits[0][0];
        next_token = sample_with_temperature(next_token_logits, temperature, top_k);
        generated_tokens.push_back(next_token);
    }
    
    auto end_decode = std::chrono::high_resolution_clock::now();
    
    if (verbose) {
        auto decode_time = std::chrono::duration_cast<std::chrono::milliseconds>(end_decode - start_decode).count();
        int new_tokens = generated_tokens.size() - input_ids.size(1);
        std::cout << "\nç”Ÿæˆå®Œæˆï¼ç”Ÿæˆäº† " << new_tokens << " ä¸ªæ–°tokens";
        if (decode_time > 0) {
            std::cout << " (é€Ÿåº¦: " << (new_tokens * 1000.0 / decode_time) << " tokens/s)";
        }
        std::cout << std::endl;
    }
    
    return generated_tokens;
}

int main() {
    try {
        qwen::ensure_required_paths(WEIGHT_PATH, TOKENIZER_SCRIPT, TOKENIZER_MODEL_DIR);
    } catch (const std::exception& e) {
        std::cerr << "âŒ è·¯å¾„é…ç½®é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }

    std::cout << std::string(70, '=') << std::endl;
    std::cout << "=== Qwen 2.5 æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===" << std::endl;
    std::cout << std::string(70, '=') << std::endl;
    
    // è®¾ç½®éšæœºç§å­
    srand(time(nullptr));
    
    // 1. åˆå§‹åŒ–è®¾å¤‡
    torch::Device device = torch::kCPU;
    if (torch::cuda::is_available()) {
        device = torch::Device(torch::kCUDA, 0);
        std::cout << "âœ… ä½¿ç”¨CUDAè®¾å¤‡" << std::endl;
    } else {
        std::cout << "â„¹ï¸ ä½¿ç”¨CPUè®¾å¤‡" << std::endl;
    }

    // 2. åˆå§‹åŒ–æ¨¡å‹
    std::cout << "\nåˆå§‹åŒ–Qwenæ¨¡å‹..." << std::endl;
    QwenModel model = QwenModel(
        QWEN_CFG.vocab_size, QWEN_CFG.hidden_size, QWEN_CFG.num_layers,
        QWEN_CFG.num_heads, QWEN_CFG.num_kv_heads, QWEN_CFG.intermediate_size,
        QWEN_CFG.max_position_embeddings, QWEN_CFG.rope_theta,
        QWEN_CFG.rms_norm_eps, QWEN_CFG.bos_token_id, QWEN_CFG.eos_token_id
    );
    model->eval();
    std::cout << "âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ" << std::endl;

    // 3. åŠ è½½æƒé‡
    std::cout << "\næ­£åœ¨åŠ è½½æƒé‡..." << std::endl;
    auto start_load = std::chrono::high_resolution_clock::now();
    model->load_weights(WEIGHT_PATH);
    auto end_load = std::chrono::high_resolution_clock::now();
    auto load_time = std::chrono::duration_cast<std::chrono::seconds>(end_load - start_load).count();
    std::cout << "âœ… æƒé‡åŠ è½½å®Œæˆ (è€—æ—¶: " << load_time << "ç§’)" << std::endl;
    
    // 4. ç§»åŠ¨åˆ°è®¾å¤‡
    std::cout << "æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ° " << device << "..." << std::endl;
    model->to(device, torch::kBFloat16);
    std::cout << "âœ… æ¨¡å‹å·²å°±ç»ª" << std::endl;

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•1ï¼šè´ªå©ªè§£ç ç”Ÿæˆã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // æµ‹è¯•1ï¼šè´ªå©ªè§£ç ï¼ˆtemperature=0ï¼‰
    {
        std::string prompt = "ä½ å¥½";
        std::cout << "\nè¾“å…¥: \"" << prompt << "\"" << std::endl;
        
        std::vector<int64_t> input_tokens = encode_text(prompt);
        torch::Tensor input_ids = torch::from_blob(
            input_tokens.data(),
            {1, static_cast<long>(input_tokens.size())},
            torch::kInt64
        ).clone().to(device);
        
        std::vector<int64_t> output_tokens = generate_text(
            model, input_ids, 20, 0.0, 0, QWEN_CFG.eos_token_id, true
        );
        
        std::string output_text = decode_tokens(output_tokens);
        std::cout << "\nå®Œæ•´è¾“å‡º:\n" << output_text << std::endl;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•2ï¼šå¸¦Temperatureçš„éšæœºé‡‡æ ·ã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // æµ‹è¯•2ï¼šéšæœºé‡‡æ ·ï¼ˆtemperature=0.7ï¼‰
    {
        std::string prompt = "ä»Šå¤©å¤©æ°”";
        std::cout << "\nè¾“å…¥: \"" << prompt << "\"" << std::endl;
        std::cout << "å‚æ•°: temperature=0.7, top_k=50\n" << std::endl;
        
        std::vector<int64_t> input_tokens = encode_text(prompt);
        torch::Tensor input_ids = torch::from_blob(
            input_tokens.data(),
            {1, static_cast<long>(input_tokens.size())},
            torch::kInt64
        ).clone().to(device);
        
        std::vector<int64_t> output_tokens = generate_text(
            model, input_ids, 30, 0.7, 50, QWEN_CFG.eos_token_id, true
        );
        
        std::string output_text = decode_tokens(output_tokens);
        std::cout << "\nå®Œæ•´è¾“å‡º:\n" << output_text << std::endl;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•3ï¼šæ›´é•¿çš„ç”Ÿæˆã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // æµ‹è¯•3ï¼šæ›´é•¿çš„ç”Ÿæˆ
    {
        std::string prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯";
        std::cout << "\nè¾“å…¥: \"" << prompt << "\"" << std::endl;
        std::cout << "å‚æ•°: temperature=0.8, top_k=40, max_tokens=50\n" << std::endl;
        
        std::vector<int64_t> input_tokens = encode_text(prompt);
        torch::Tensor input_ids = torch::from_blob(
            input_tokens.data(),
            {1, static_cast<long>(input_tokens.size())},
            torch::kInt64
        ).clone().to(device);
        
        std::vector<int64_t> output_tokens = generate_text(
            model, input_ids, 50, 0.8, 40, QWEN_CFG.eos_token_id, true
        );
        
        std::string output_text = decode_tokens(output_tokens);
        std::cout << "\nå®Œæ•´è¾“å‡º:\n" << output_text << std::endl;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•4ï¼šå¯¹æ¯”ä¸åŒtemperatureã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // æµ‹è¯•4ï¼šå¯¹æ¯”ä¸åŒtemperature
    {
        std::string prompt = "åŒ—äº¬æ˜¯";
        std::cout << "\nè¾“å…¥: \"" << prompt << "\"" << std::endl;
        
        std::vector<int64_t> input_tokens = encode_text(prompt);
        
        for (float temp : {0.1f, 0.5f, 1.0f}) {
            std::cout << "\n--- Temperature = " << temp << " ---" << std::endl;
            
            torch::Tensor input_ids = torch::from_blob(
                input_tokens.data(),
                {1, static_cast<long>(input_tokens.size())},
                torch::kInt64
            ).clone().to(device);
            
            std::vector<int64_t> output_tokens = generate_text(
                model, input_ids, 15, temp, 50, QWEN_CFG.eos_token_id, false
            );
            
            std::string output_text = decode_tokens(output_tokens);
            std::cout << "è¾“å‡º: " << output_text << std::endl;
        }
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ğŸ‰ æ‰€æœ‰ç”Ÿæˆæµ‹è¯•å®Œæˆï¼" << std::endl;
    std::cout << std::string(70, '=') << std::endl;
    
    return 0;
}
