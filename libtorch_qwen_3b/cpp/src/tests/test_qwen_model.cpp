#include "../include/qwen_model.h"
#include "../include/qwen_tokenizer.h"
#include "../include/qwen_env.h"
#include "../include/qwen_model_config.h"
#include <iostream>
#include <vector>
#include <chrono>

// é…ç½®Qwenæ¨¡å‹å‚æ•°ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
const auto& QWEN_CFG = qwen::get_model_config();
const std::string WEIGHT_PATH = qwen::get_weight_path();
const std::string TOKENIZER_SCRIPT = qwen::get_tokenizer_script();
const std::string TOKENIZER_MODEL_DIR = qwen::get_tokenizer_model_dir();
const std::string PYTHON_CMD = qwen::get_python_cmd();

// æ‰“å°å¼ é‡ä¿¡æ¯
void print_tensor_info(const std::string& tensor_name, const torch::Tensor& tensor) {
    std::cout << "\n" << tensor_name << "ï¼š" << std::endl;
    std::cout << "  å½¢çŠ¶ï¼š" << tensor.sizes() << std::endl;
    std::cout << "  æ•°æ®ç±»å‹ï¼š" << tensor.dtype().name() << std::endl;
    std::cout << "  è®¾å¤‡ï¼š" << tensor.device() << std::endl;
    if (tensor.numel() > 0 && tensor.numel() <= 10) {
        std::cout << "  å€¼ï¼š" << tensor.flatten() << std::endl;
    } else if (tensor.numel() > 0) {
        int64_t print_size = std::min(5L, tensor.numel());
        std::cout << "  å‰" << print_size << "ä¸ªå…ƒç´ ï¼š" << tensor.flatten().slice(0, 0, print_size) << std::endl;
    }
}

// è§£ç token IDsä¸ºæ–‡æœ¬ï¼ˆé€šè¿‡Python tokenizerï¼‰
std::string decode_tokens(const std::vector<int64_t>& token_ids) {
    // æ„å»ºPythonå‘½ä»¤
    std::string ids_str = "[";
    for (size_t i = 0; i < token_ids.size(); ++i) {
        ids_str += std::to_string(token_ids[i]);
        if (i < token_ids.size() - 1) ids_str += ",";
    }
    ids_str += "]";
    
    std::string cmd = PYTHON_CMD + " -c \"from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('" + TOKENIZER_MODEL_DIR + "', trust_remote_code=True); print(tokenizer.decode(" + ids_str + "))\" 2>/dev/null";
    
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) return "[è§£ç å¤±è´¥]";
    
    char buffer[4096];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    pclose(pipe);
    
    // ç§»é™¤æœ«å°¾æ¢è¡Œç¬¦
    if (!result.empty() && result.back() == '\n') {
        result.pop_back();
    }
    
    return result;
}

int main() {
    std::cout << std::string(70, '=') << std::endl;
    std::cout << "=== Qwen 2.5 å®Œæ•´æ¨¡å‹æµ‹è¯• ===" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    try {
        qwen::ensure_required_paths(WEIGHT_PATH, TOKENIZER_SCRIPT, TOKENIZER_MODEL_DIR);
    } catch (const std::exception& e) {
        std::cerr << "âŒ è·¯å¾„é…ç½®é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
    
    // 1. åˆå§‹åŒ–è®¾å¤‡
    torch::Device device = torch::kCPU;
    if (torch::cuda::is_available()) {
        device = torch::Device(torch::kCUDA, 0);
        std::cout << "âœ… ä½¿ç”¨CUDAè®¾å¤‡" << std::endl;
    } else {
        std::cout << "â„¹ï¸ ä½¿ç”¨CPUè®¾å¤‡" << std::endl;
    }

    // 2. åˆå§‹åŒ–æ¨¡å‹
    std::cout << "\nåˆå§‹åŒ–Qwenæ¨¡å‹..." << std::endl;
    QwenModel model = QwenModel(
        QWEN_CFG.vocab_size,
        QWEN_CFG.hidden_size,
        QWEN_CFG.num_layers,
        QWEN_CFG.num_heads,
        QWEN_CFG.num_kv_heads,
        QWEN_CFG.intermediate_size,
        QWEN_CFG.max_position_embeddings,
        QWEN_CFG.rope_theta,
        QWEN_CFG.rms_norm_eps,
        QWEN_CFG.bos_token_id,
        QWEN_CFG.eos_token_id
    );
    model->eval();
    std::cout << "âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ" << std::endl;
    std::cout << "  æ¨¡å‹é…ç½®:" << std::endl;
    std::cout << "    - å±‚æ•°: " << QWEN_CFG.num_layers << std::endl;
    std::cout << "    - éšè—ç»´åº¦: " << QWEN_CFG.hidden_size << std::endl;
    std::cout << "    - ä¸­é—´ç»´åº¦: " << QWEN_CFG.intermediate_size << std::endl;
    std::cout << "    - æ³¨æ„åŠ›å¤´æ•°: " << QWEN_CFG.num_heads << " (Q) / " << QWEN_CFG.num_kv_heads << " (KV)" << std::endl;
    std::cout << "    - è¯æ±‡è¡¨å¤§å°: " << QWEN_CFG.vocab_size << std::endl;

    // 3. åŠ è½½æƒé‡
    auto start_load = std::chrono::high_resolution_clock::now();
    model->load_weights(WEIGHT_PATH);
    auto end_load = std::chrono::high_resolution_clock::now();
    auto load_time = std::chrono::duration_cast<std::chrono::seconds>(end_load - start_load).count();
    std::cout << "æƒé‡åŠ è½½è€—æ—¶: " << load_time << " ç§’" << std::endl;
    
    // 4. ç§»åŠ¨åˆ°è®¾å¤‡
    std::cout << "\næ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ° " << device << "..." << std::endl;
    auto start_move = std::chrono::high_resolution_clock::now();
    model->to(device, torch::kBFloat16);
    auto end_move = std::chrono::high_resolution_clock::now();
    auto move_time = std::chrono::duration_cast<std::chrono::seconds>(end_move - start_move).count();
    std::cout << "âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆè€—æ—¶: " << move_time << " ç§’ï¼‰" << std::endl;

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•1ï¼šåŸºæœ¬å‰å‘ä¼ æ’­ã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // 5. æµ‹è¯•å‰å‘ä¼ æ’­
    std::vector<int64_t> test_tokens = {100, 200, 300};
    torch::Tensor input_ids = torch::from_blob(
        test_tokens.data(), 
        {1, static_cast<long>(test_tokens.size())}, 
        torch::kInt64
    ).clone().to(device);
    print_tensor_info("è¾“å…¥Token IDs", input_ids);
    
    torch::Tensor logits;
    {
        torch::NoGradGuard no_grad;
        auto start_fwd = std::chrono::high_resolution_clock::now();
        logits = model->forward(input_ids, false);
        auto end_fwd = std::chrono::high_resolution_clock::now();
        auto fwd_time = std::chrono::duration_cast<std::chrono::milliseconds>(end_fwd - start_fwd).count();
        
        print_tensor_info("æ¨¡å‹è¾“å‡ºlogits", logits);
        std::cout << "å‰å‘ä¼ æ’­è€—æ—¶: " << fwd_time << " ms" << std::endl;
    }
    
    // éªŒè¯è¾“å‡ºå½¢çŠ¶
    if (logits.size(0) == 1 && logits.size(1) == 3 && logits.size(2) == QWEN_CFG.vocab_size) {
        std::cout << "âœ… è¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡: [batch_size=1, seq_len=3, vocab_size=" << QWEN_CFG.vocab_size << "]" << std::endl;
    } else {
        std::cerr << "âŒ è¾“å‡ºå½¢çŠ¶éªŒè¯å¤±è´¥" << std::endl;
        return 1;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•2ï¼šæ–‡æœ¬ç”Ÿæˆï¼ˆç®€å•ç¤ºä¾‹ï¼‰ã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // 6. åˆå§‹åŒ–tokenizer
    QwenTokenizer tokenizer(TOKENIZER_SCRIPT);
    
    // 7. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    std::string prompt = "ä½ å¥½";
    std::cout << "\nè¾“å…¥æç¤º: \"" << prompt << "\"" << std::endl;
    
    try {
        // åˆ†è¯
        torch::Tensor prompt_ids = tokenizer.encode(prompt).to(device);
        std::cout << "è¾“å…¥Token IDs: " << prompt_ids << std::endl;
        
        // ç”Ÿæˆ
        std::cout << "\nå¼€å§‹ç”Ÿæˆï¼ˆæœ€å¤š20ä¸ªtokenï¼‰";
        auto start_gen = std::chrono::high_resolution_clock::now();
        std::vector<int64_t> generated_ids = model->generate(prompt_ids, 20);
        auto end_gen = std::chrono::high_resolution_clock::now();
        auto gen_time = std::chrono::duration_cast<std::chrono::milliseconds>(end_gen - start_gen).count();
        
        std::cout << "\n\nç”Ÿæˆå®Œæˆï¼" << std::endl;
        std::cout << "ç”ŸæˆTokenæ•°: " << generated_ids.size() << std::endl;
        std::cout << "ç”Ÿæˆè€—æ—¶: " << gen_time << " ms" << std::endl;
        std::cout << "å¹³å‡é€Ÿåº¦: " << (generated_ids.size() * 1000.0 / gen_time) << " tokens/s" << std::endl;
        
        // è§£ç 
        std::cout << "\nç”Ÿæˆçš„Token IDs: [";
        for (size_t i = 0; i < std::min(generated_ids.size(), size_t(20)); ++i) {
            std::cout << generated_ids[i];
            if (i < generated_ids.size() - 1) std::cout << ", ";
        }
        if (generated_ids.size() > 20) std::cout << "...";
        std::cout << "]" << std::endl;
        
        std::cout << "\nè§£ç ç”Ÿæˆçš„æ–‡æœ¬..." << std::endl;
        std::string generated_text = decode_tokens(generated_ids);
        std::cout << "\n" << std::string(70, '-') << std::endl;
        std::cout << "å®Œæ•´è¾“å‡º:\n" << generated_text << std::endl;
        std::cout << std::string(70, '-') << std::endl;
        
        std::cout << "\nâœ… æ–‡æœ¬ç”Ÿæˆæµ‹è¯•æˆåŠŸ" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: " << e.what() << std::endl;
        return 1;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•3ï¼šæ›´é•¿çš„æç¤ºè¯ã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // 8. æµ‹è¯•æ›´å¤æ‚çš„æç¤ºè¯
    std::string long_prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯";
    std::cout << "\nè¾“å…¥æç¤º: \"" << long_prompt << "\"" << std::endl;
    
    try {
        torch::Tensor long_prompt_ids = tokenizer.encode(long_prompt).to(device);
        std::cout << "è¾“å…¥Token IDsæ•°é‡: " << long_prompt_ids.size(0) << std::endl;
        
        std::cout << "\nå¼€å§‹ç”Ÿæˆï¼ˆæœ€å¤š30ä¸ªtokenï¼‰";
        auto start_gen2 = std::chrono::high_resolution_clock::now();
        std::vector<int64_t> generated_ids2 = model->generate(long_prompt_ids, 30);
        auto end_gen2 = std::chrono::high_resolution_clock::now();
        auto gen_time2 = std::chrono::duration_cast<std::chrono::milliseconds>(end_gen2 - start_gen2).count();
        
        std::cout << "\n\nç”Ÿæˆå®Œæˆï¼" << std::endl;
        std::cout << "ç”Ÿæˆè€—æ—¶: " << gen_time2 << " ms" << std::endl;
        
        std::string generated_text2 = decode_tokens(generated_ids2);
        std::cout << "\n" << std::string(70, '-') << std::endl;
        std::cout << "å®Œæ•´è¾“å‡º:\n" << generated_text2 << std::endl;
        std::cout << std::string(70, '-') << std::endl;
        
        std::cout << "\nâœ… é•¿æç¤ºè¯ç”Ÿæˆæµ‹è¯•æˆåŠŸ" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ é•¿æç¤ºè¯ç”Ÿæˆå¤±è´¥: " << e.what() << std::endl;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ğŸ‰ Qwen 2.5å®Œæ•´æ¨¡å‹æµ‹è¯•å®Œæˆ" << std::endl;
    std::cout << std::string(70, '=') << std::endl;
    
    return 0;
}
