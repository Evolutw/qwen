#include "../include/qwen_model.h"
#include "../include/qwen_env.h"
#include "../include/qwen_model_config.h"
#include <iostream>
#include <vector>
#include <chrono>
#include <algorithm>
#include <random>
#include <unordered_set>

// é…ç½®Qwenæ¨¡å‹å‚æ•°ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
const auto& QWEN_CFG = qwen::get_model_config();
const int64_t QWEN_EOS_TOKEN_ID = QWEN_CFG.eos_token_id;
const int64_t QWEN_IM_START_ID = 151644;  // <|im_start|>
const int64_t QWEN_IM_END_ID = QWEN_CFG.im_end_id;     // <|im_end|> (same as EOS)
const int64_t QWEN_ASSISTANT_TOKEN_ID = 77091; // token for "assistant" in ChatML
const std::string WEIGHT_PATH = qwen::get_weight_path();
const std::string TOKENIZER_SCRIPT = qwen::get_tokenizer_script();
const std::string TOKENIZER_MODEL_DIR = qwen::get_tokenizer_model_dir();
const std::string PYTHON_CMD = qwen::get_python_cmd();

// ç¼–ç æ–‡æœ¬ä¸ºtoken IDsï¼ˆä½¿ç”¨èŠå¤©æ¨¡æ¿ï¼‰
std::vector<int64_t> encode_chat(const std::string& user_message) {
    std::string cmd = PYTHON_CMD + " " + TOKENIZER_SCRIPT + " \"" + user_message + "\" --chat 2>/dev/null";
    
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) {
        throw std::runtime_error("æ— æ³•æ‰§è¡Œåˆ†è¯å‘½ä»¤");
    }
    
    char buffer[4096];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    pclose(pipe);
    
    // è§£æJSONæ ¼å¼çš„token IDs
    std::vector<int64_t> token_ids;
    size_t start = result.find("[");
    size_t end = result.find("]");
    if (start != std::string::npos && end != std::string::npos) {
        std::string ids_str = result.substr(start + 1, end - start - 1);
        std::stringstream ss(ids_str);
        std::string item;
        while (std::getline(ss, item, ',')) {
            token_ids.push_back(std::stoll(item));
        }
    }
    
    return token_ids;
}

// è§£ç token IDsä¸ºæ–‡æœ¬
std::string decode_tokens(const std::vector<int64_t>& token_ids) {
    if (token_ids.empty()) return "[ç©º]";
    
    std::string ids_str = "[";
    for (size_t i = 0; i < token_ids.size(); ++i) {
        ids_str += std::to_string(token_ids[i]);
        if (i < token_ids.size() - 1) ids_str += ",";
    }
    ids_str += "]";
    
    std::string cmd = PYTHON_CMD + " -c \"from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('" + TOKENIZER_MODEL_DIR + "', trust_remote_code=True); print(tokenizer.decode(" + ids_str + "), end='')\" 2>/dev/null";
    
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) return "[è§£ç å¤±è´¥]";
    
    char buffer[4096];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    pclose(pipe);
    
    return result.empty() ? "[ç©º]" : result;
}

// å¦‚æœç”Ÿæˆåˆ°æ–°çš„assistantèµ·å§‹æ ‡è®°ï¼ŒåŠæ—¶åœæ­¢
bool is_new_assistant_turn(const std::vector<int64_t>& tokens) {
    if (tokens.empty()) return false;
    if (tokens.back() == QWEN_IM_START_ID) return true;
    if (tokens.size() >= 2 && tokens[tokens.size() - 2] == QWEN_IM_START_ID && tokens.back() == QWEN_ASSISTANT_TOKEN_ID) {
        return true;
    }
    if (tokens.size() >= 3 && tokens[tokens.size() - 3] == QWEN_IM_START_ID && tokens[tokens.size() - 2] == QWEN_ASSISTANT_TOKEN_ID && tokens.back() == 198) {
        return true;
    }
    return false;
}

// å»é™¤ç‰¹æ®Šæ ‡è®°åçš„ç”Ÿæˆå†…å®¹
std::vector<int64_t> trim_at_special_tokens(const std::vector<int64_t>& tokens) {
    size_t cut = tokens.size();
    for (size_t i = 0; i < tokens.size(); ++i) {
        if (tokens[i] == QWEN_IM_END_ID || tokens[i] == QWEN_IM_START_ID) {
            cut = i;
            break;
        }
    }
    return std::vector<int64_t>(tokens.begin(), tokens.begin() + cut);
}

// Temperatureé‡‡æ ·ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
int64_t sample_with_temperature(const torch::Tensor& logits, float temperature = 1.0, int top_k = 50) {
    torch::Tensor probs;
    
    if (temperature <= 0.0001) {
        // temperatureæ¥è¿‘0ï¼Œä½¿ç”¨è´ªå©ªé‡‡æ ·
        return logits.argmax(-1).item<int64_t>();
    }
    
    // åº”ç”¨temperature
    torch::Tensor scaled_logits = logits / temperature;
    
    // Top-ké‡‡æ ·
    if (top_k > 0 && top_k < logits.size(-1)) {
        auto topk_result = torch::topk(scaled_logits, top_k, -1);
        auto topk_values = std::get<0>(topk_result);
        auto topk_indices = std::get<1>(topk_result);
        
        // è®¡ç®—æ¦‚ç‡
        probs = torch::softmax(topk_values, -1);
        
        // é‡‡æ ·
        torch::Tensor cumsum = torch::cumsum(probs, -1);
        float random_val = static_cast<float>(rand()) / RAND_MAX;
        
        for (int i = 0; i < top_k; ++i) {
            if (cumsum[i].item<float>() >= random_val) {
                return topk_indices[i].item<int64_t>();
            }
        }
        return topk_indices[-1].item<int64_t>();
    } else {
        // æ ‡å‡†softmaxé‡‡æ ·
        probs = torch::softmax(scaled_logits, -1);
        torch::Tensor cumsum = torch::cumsum(probs, -1);
        float random_val = static_cast<float>(rand()) / RAND_MAX;
        
        for (int i = 0; i < probs.size(-1); ++i) {
            if (cumsum[i].item<float>() >= random_val) {
                return i;
            }
        }
        return probs.size(-1) - 1;
    }
}

// åº”ç”¨repetition penaltyï¼ˆæƒ©ç½šå·²ç”Ÿæˆçš„tokenï¼‰
torch::Tensor apply_repetition_penalty(torch::Tensor logits,
                                       const std::vector<int64_t>& generated_tokens,
                                       float penalty = 1.1f) {
    if (penalty <= 1.0f || generated_tokens.empty()) {
        return logits;
    }

    logits = logits.clone();
    for (int64_t token : generated_tokens) {
        float current_logit = logits[token].item<float>();
        if (current_logit > 0) {
            logits[token] = current_logit / penalty;
        } else {
            logits[token] = current_logit * penalty;
        }
    }
    return logits;
}

// ç¦æ­¢é‡å¤n-gramï¼ˆé»˜è®¤3-gramï¼‰
torch::Tensor apply_no_repeat_ngram(torch::Tensor logits,
                                    const std::vector<int64_t>& generated_tokens,
                                    int no_repeat_ngram_size = 3) {
    if (no_repeat_ngram_size <= 1) return logits;
    if (generated_tokens.size() < static_cast<size_t>(no_repeat_ngram_size - 1)) return logits;

    logits = logits.clone();

    const int n = no_repeat_ngram_size;
    const size_t prefix_start = generated_tokens.size() - (n - 1);
    std::vector<int64_t> prefix(generated_tokens.begin() + prefix_start, generated_tokens.end());

    // æ‰¾åˆ°æ‰€æœ‰ä¸prefixåŒ¹é…çš„n-gramï¼Œå¹¶ç¦æ­¢å…¶ä¸‹ä¸€token
    for (size_t i = 0; i + n <= generated_tokens.size(); ++i) {
        bool match = true;
        for (int j = 0; j < n - 1; ++j) {
            if (generated_tokens[i + j] != prefix[j]) {
                match = false;
                break;
            }
        }
        if (match) {
            int64_t banned_token = generated_tokens[i + (n - 1)];
            logits[banned_token] = -1e9;
        }
    }

    return logits;
}

// ç®€å•é‡å¤æ£€æµ‹ï¼šæœ€è¿‘çª—å£å†…é‡å¤æ¯”ä¾‹è¿‡é«˜åˆ™æå‰åœæ­¢
bool should_stop_on_repetition(const std::vector<int64_t>& generated_tokens,
                               size_t window = 30,
                               float min_unique_ratio = 0.35f) {
    if (generated_tokens.size() < window) return false;
    std::unordered_set<int64_t> uniq;
    for (size_t i = generated_tokens.size() - window; i < generated_tokens.size(); ++i) {
        uniq.insert(generated_tokens[i]);
    }
    float ratio = static_cast<float>(uniq.size()) / static_cast<float>(window);
    return ratio < min_unique_ratio;
}

// èŠå¤©ç”Ÿæˆå‡½æ•°
std::string chat(
    QwenModel& model,
    const std::string& user_message,
    int max_new_tokens = 100,
    float temperature = 0.7,
    int top_k = 50,
    float repetition_penalty = 1.3f,
    int no_repeat_ngram_size = 4,
    bool verbose = true
) {
    // ç¼–ç ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«èŠå¤©æ¨¡æ¿ï¼‰
    if (verbose) {
        std::cout << "\nç”¨æˆ·: " << user_message << std::endl;
        std::cout << "åŠ©æ‰‹: " << std::flush;
    }
    
    std::vector<int64_t> input_tokens = encode_chat(user_message);
    if (input_tokens.empty()) {
        return "[ç¼–ç å¤±è´¥]";
    }
    
    // è½¬æ¢ä¸ºtensor
    torch::Tensor input_ids = torch::from_blob(
        input_tokens.data(),
        {1, static_cast<long>(input_tokens.size())},
        torch::kInt64
    ).clone().to(model->parameters()[0].device());
    
    // æ¸…ç©ºKVç¼“å­˜
    model->clear_cache();
    
    torch::NoGradGuard no_grad;
    
    // Prefillé˜¶æ®µ
    auto start_time = std::chrono::high_resolution_clock::now();
    torch::Tensor logits = model->forward(input_ids, true);
    
    std::vector<int64_t> generated_tokens;
    // é‡‡æ ·ç¬¬ä¸€ä¸ªtoken
    torch::Tensor next_token_logits = logits[0][-1];
    next_token_logits = apply_repetition_penalty(next_token_logits, generated_tokens, repetition_penalty);
    next_token_logits = apply_no_repeat_ngram(next_token_logits, generated_tokens, no_repeat_ngram_size);

    int64_t next_token = sample_with_temperature(next_token_logits, temperature, top_k);

    generated_tokens.push_back(next_token);
    
    // è‡ªå›å½’ç”Ÿæˆ
    for (int i = 1; i < max_new_tokens; ++i) {
        // æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸæ ‡è®°
        if (next_token == QWEN_IM_END_ID || next_token == QWEN_EOS_TOKEN_ID) {
            if (verbose) std::cout << " [åœæ­¢]" << std::flush;
            break;
        }
        
        if (should_stop_on_repetition(generated_tokens)) {
            if (verbose) std::cout << " [é‡å¤åœæ­¢]" << std::flush;
            break;
        }
        
        // ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
        torch::Tensor next_input = torch::tensor({{next_token}}, torch::kInt64).to(input_ids.device());
        logits = model->forward(next_input, true);
        next_token_logits = logits[0][0];
        next_token_logits = apply_repetition_penalty(next_token_logits, generated_tokens, repetition_penalty);
        next_token_logits = apply_no_repeat_ngram(next_token_logits, generated_tokens, no_repeat_ngram_size);

        next_token = sample_with_temperature(next_token_logits, temperature, top_k);
        generated_tokens.push_back(next_token);

        if (is_new_assistant_turn(generated_tokens)) {
            if (verbose) std::cout << " [å¯¹è¯ç»“æŸ]" << std::flush;
            break;
        }
    }
    
    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time).count();
    
    // è§£ç å®Œæ•´å›å¤
    std::vector<int64_t> trimmed_tokens = trim_at_special_tokens(generated_tokens);
    std::string response = decode_tokens(trimmed_tokens);
    
    // ç§»é™¤ç‰¹æ®Šæ ‡è®°
    size_t pos = response.find("<|im_end|>");
    if (pos != std::string::npos) {
        response = response.substr(0, pos);
    }
    
    if (verbose) {
        std::cout << response << std::endl;
        std::cout << "\nâ±ï¸ ç”Ÿæˆæ—¶é—´: " << duration << "ms";
        std::cout << " | ç”Ÿæˆtokens: " << generated_tokens.size();
        if (duration > 0) {
            std::cout << " | é€Ÿåº¦: " << (generated_tokens.size() * 1000.0 / duration) << " tokens/s";
        }
        std::cout << std::endl;
    }
    
    return response;
}

int main() {
    std::cout << std::string(70, '=') << std::endl;
    std::cout << "=== Qwen 2.5 èŠå¤©æµ‹è¯•ï¼ˆä½¿ç”¨ChatMLæ ¼å¼ï¼‰===" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    try {
        qwen::ensure_required_paths(WEIGHT_PATH, TOKENIZER_SCRIPT, TOKENIZER_MODEL_DIR);
    } catch (const std::exception& e) {
        std::cerr << "âŒ è·¯å¾„é…ç½®é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
    
    // è®¾ç½®éšæœºç§å­
    srand(time(nullptr));
    
    // 1. åˆå§‹åŒ–è®¾å¤‡
    torch::Device device = torch::kCPU;
    if (torch::cuda::is_available()) {
        device = torch::Device(torch::kCUDA, 0);
        std::cout << "âœ… ä½¿ç”¨CUDAè®¾å¤‡" << std::endl;
    } else {
        std::cout << "â„¹ï¸ ä½¿ç”¨CPUè®¾å¤‡" << std::endl;
    }

    // 2. åˆå§‹åŒ–æ¨¡å‹
    std::cout << "\nåˆå§‹åŒ–Qwenæ¨¡å‹..." << std::endl;
    QwenModel model = QwenModel(
        QWEN_CFG.vocab_size, QWEN_CFG.hidden_size, QWEN_CFG.num_layers,
        QWEN_CFG.num_heads, QWEN_CFG.num_kv_heads, QWEN_CFG.intermediate_size,
        QWEN_CFG.max_position_embeddings, QWEN_CFG.rope_theta,
        QWEN_CFG.rms_norm_eps, QWEN_CFG.bos_token_id, QWEN_CFG.eos_token_id
    );
    model->eval();
    std::cout << "âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ" << std::endl;

    // 3. åŠ è½½æƒé‡
    std::cout << "\næ­£åœ¨åŠ è½½æƒé‡..." << std::endl;
    auto start_load = std::chrono::high_resolution_clock::now();
    model->load_weights(WEIGHT_PATH);
    auto end_load = std::chrono::high_resolution_clock::now();
    auto load_time = std::chrono::duration_cast<std::chrono::seconds>(end_load - start_load).count();
    std::cout << "âœ… æƒé‡åŠ è½½å®Œæˆ (è€—æ—¶: " << load_time << "ç§’)" << std::endl;
    
    // 4. ç§»åŠ¨åˆ°è®¾å¤‡
    std::cout << "æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ° " << device << "..." << std::endl;
    model->to(device, torch::kBFloat16);
    std::cout << "âœ… æ¨¡å‹å·²å°±ç»ª" << std::endl;

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ã€å¯¹è¯æµ‹è¯• - ä½¿ç”¨ChatMLæ ¼å¼è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‘" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    // æµ‹è¯•1ï¼šç®€å•é—®å€™
    std::cout << "\n[æµ‹è¯• 1/5] ç®€å•é—®å€™" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    chat(model, "ä½ å¥½", 50, 0.7, 50, true);

    // æµ‹è¯•2ï¼šçŸ¥è¯†é—®ç­”
    std::cout << "\n\n[æµ‹è¯• 2/5] çŸ¥è¯†é—®ç­”" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    chat(model, "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", 100, 0.7, 50, true);

    // æµ‹è¯•3ï¼šåˆ›æ„å†™ä½œ
    std::cout << "\n\n[æµ‹è¯• 3/5] åˆ›æ„å†™ä½œï¼ˆé«˜temperatureï¼‰" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    chat(model, "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", 80, 0.9, 40, true);

    // æµ‹è¯•4ï¼šäº‹å®å›ç­”ï¼ˆä½temperatureï¼‰
    std::cout << "\n\n[æµ‹è¯• 4/5] äº‹å®å›ç­”ï¼ˆä½temperatureï¼‰" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    chat(model, "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½å—ï¼Ÿ", 50, 0.3, 50, true);

    // æµ‹è¯•5ï¼šç®€çŸ­é—®ç­”
    std::cout << "\n\n[æµ‹è¯• 5/5] ç®€çŸ­é—®ç­”" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    chat(model, "1+1=?", 20, 0.1, 50, true);

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "ğŸ‰ æ‰€æœ‰èŠå¤©æµ‹è¯•å®Œæˆï¼" << std::endl;
    std::cout << std::string(70, '=') << std::endl;
    
    return 0;
}
