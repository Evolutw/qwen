#include "../include/qwen_attention.h"
#include "../include/qwen_embedding.h"
#include "../include/qwen_env.h"
#include "../include/qwen_model_config.h"
#include <iostream>
#include <vector>

// é…ç½®Qwenæ¨¡å‹å‚æ•°ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
const auto& QWEN_CFG = qwen::get_model_config();
const std::string WEIGHT_PATH = qwen::get_weight_path();

// æ‰“å°å¼ é‡ä¿¡æ¯
void print_tensor_info(const std::string& tensor_name, const torch::Tensor& tensor) {
    std::cout << "\n" << tensor_name << "ï¼š" << std::endl;
    std::cout << "  å½¢çŠ¶ï¼š" << tensor.sizes() << std::endl;
    std::cout << "  æ•°æ®ç±»å‹ï¼š" << tensor.dtype().name() << std::endl;
    std::cout << "  è®¾å¤‡ï¼š" << tensor.device() << std::endl;
    if (tensor.numel() > 0) {
        int64_t print_size = std::min(5L, tensor.numel());
        std::cout << "  å‰" << print_size << "ä¸ªå…ƒç´ ï¼š" << tensor.flatten().slice(0, 0, print_size) << std::endl;
    }
}

int main() {
    std::cout << "=== Qwen Attentionå±‚æµ‹è¯• ===" << std::endl;

    try {
        qwen::ensure_required_paths(WEIGHT_PATH, qwen::get_tokenizer_script(), qwen::get_tokenizer_model_dir());
    } catch (const std::exception& e) {
        std::cerr << "âŒ è·¯å¾„é…ç½®é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
    
    // 1. åˆå§‹åŒ–è®¾å¤‡
    torch::Device device = torch::kCPU;
    if (torch::cuda::is_available()) {
        device = torch::Device(torch::kCUDA, 0);
        std::cout << "âœ… ä½¿ç”¨CUDAè®¾å¤‡" << std::endl;
    } else {
        std::cout << "â„¹ï¸ ä½¿ç”¨CPUè®¾å¤‡" << std::endl;
    }

    // 2. åˆå§‹åŒ–Attentionå±‚
    QwenAttention attention = QwenAttention(
        QWEN_CFG.hidden_size,
        QWEN_CFG.num_heads,
        QWEN_CFG.num_kv_heads,
        QWEN_CFG.max_position_embeddings,
        QWEN_CFG.rope_theta
    );
    attention->eval();
    std::cout << "âœ… Attentionå±‚åˆå§‹åŒ–å®Œæˆ" << std::endl;
    std::cout << "  hidden_size: " << QWEN_CFG.hidden_size << std::endl;
    std::cout << "  num_heads: " << QWEN_CFG.num_heads << std::endl;
    std::cout << "  num_kv_heads: " << QWEN_CFG.num_kv_heads << " (GQA)" << std::endl;
    std::cout << "  head_dim: " << (QWEN_CFG.hidden_size / QWEN_CFG.num_heads) << std::endl;

    // 3. åŠ è½½æƒé‡ï¼ˆä½¿ç”¨ç¬¬0å±‚çš„attentionæƒé‡ï¼‰
    attention->load_weights(WEIGHT_PATH, 0);
    
    // 4. ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è½¬æ¢æ•°æ®ç±»å‹
    attention->to(device, torch::kBFloat16);
    std::cout << "âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡ï¼š" << device << std::endl;

    std::cout << "\n" << std::string(60, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•1ï¼šåŸºæœ¬å‰å‘ä¼ æ’­ã€‘" << std::endl;
    std::cout << std::string(60, '=') << std::endl;

    // 5. ç”Ÿæˆæµ‹è¯•è¾“å…¥
    // æ¨¡æ‹ŸEmbeddingå±‚çš„è¾“å‡ºï¼š[batch_size=2, seq_len=4, hidden_size=896]
    int64_t batch_size = 2;
    int64_t seq_len = 4;
    torch::Tensor hidden_states = torch::randn(
        {batch_size, seq_len, QWEN_CFG.hidden_size},
        torch::TensorOptions().dtype(torch::kBFloat16).device(device)
    );
    print_tensor_info("è¾“å…¥hidden_states", hidden_states);

    // 6. å‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨KVç¼“å­˜ï¼‰
    {
        torch::NoGradGuard no_grad;
        torch::Tensor output = attention->forward(hidden_states, false);
        print_tensor_info("Attentionè¾“å‡º", output);
        
        // éªŒè¯è¾“å‡ºå½¢çŠ¶
        std::vector<int64_t> expected_shape = {batch_size, seq_len, QWEN_CFG.hidden_size};
        bool shape_match = true;
        for (size_t i = 0; i < expected_shape.size(); ++i) {
            if (output.size(i) != expected_shape[i]) {
                shape_match = false;
                break;
            }
        }
        
        if (shape_match) {
            std::cout << "âœ… Attentionè¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡" << std::endl;
        } else {
            std::cerr << "âŒ Attentionè¾“å‡ºå½¢çŠ¶éªŒè¯å¤±è´¥" << std::endl;
            return 1;
        }
    }

    std::cout << "\n" << std::string(60, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•2ï¼šKVç¼“å­˜åŠŸèƒ½ã€‘" << std::endl;
    std::cout << std::string(60, '=') << std::endl;

    // 7. æµ‹è¯•KVç¼“å­˜
    attention->clear_cache();
    
    // ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼šç¼“å­˜KV
    torch::Tensor first_input = torch::randn(
        {1, 3, QWEN_CFG.hidden_size},
        torch::TensorOptions().dtype(torch::kBFloat16).device(device)
    );
    print_tensor_info("ç¬¬ä¸€æ¬¡è¾“å…¥ (seq_len=3)", first_input);
    
    torch::Tensor first_output;
    {
        torch::NoGradGuard no_grad;
        first_output = attention->forward(first_input, true);
        print_tensor_info("ç¬¬ä¸€æ¬¡è¾“å‡º", first_output);
    }
    
    // ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­ï¼šä½¿ç”¨ç¼“å­˜çš„KVï¼Œåªå¤„ç†æ–°token
    torch::Tensor second_input = torch::randn(
        {1, 1, QWEN_CFG.hidden_size},
        torch::TensorOptions().dtype(torch::kBFloat16).device(device)
    );
    print_tensor_info("ç¬¬äºŒæ¬¡è¾“å…¥ (seq_len=1, ä½¿ç”¨KVç¼“å­˜)", second_input);
    
    torch::Tensor second_output;
    {
        torch::NoGradGuard no_grad;
        second_output = attention->forward(second_input, true);
        print_tensor_info("ç¬¬äºŒæ¬¡è¾“å‡º", second_output);
    }
    
    std::cout << "\nâœ… KVç¼“å­˜æµ‹è¯•å®Œæˆ" << std::endl;
    std::cout << "  ç¬¬ä¸€æ¬¡è¾“å…¥é•¿åº¦: 3, è¾“å‡ºé•¿åº¦: " << first_output.size(1) << std::endl;
    std::cout << "  ç¬¬äºŒæ¬¡è¾“å…¥é•¿åº¦: 1, è¾“å‡ºé•¿åº¦: " << second_output.size(1) << std::endl;
    std::cout << "  è¯´æ˜: ç¬¬äºŒæ¬¡ä½¿ç”¨äº†ç¼“å­˜çš„KVï¼Œåªéœ€å¤„ç†1ä¸ªæ–°token" << std::endl;

    std::cout << "\n" << std::string(60, '=') << std::endl;
    std::cout << "ã€æµ‹è¯•3ï¼šå®Œæ•´æµç¨‹ï¼ˆEmbedding + Attentionï¼‰ã€‘" << std::endl;
    std::cout << std::string(60, '=') << std::endl;

    // 8. ç»“åˆEmbeddingå±‚æµ‹è¯•å®Œæ•´æµç¨‹
    QwenEmbedding embedding = QwenEmbedding(QWEN_CFG.vocab_size, QWEN_CFG.hidden_size);
    embedding->eval();
    embedding->load_weights(WEIGHT_PATH);
    embedding->to(device, torch::kBFloat16);
    std::cout << "âœ… Embeddingå±‚å·²åŠ è½½" << std::endl;
    
    // ç”Ÿæˆtoken IDs
    std::vector<int64_t> token_ids = {100, 200, 300, 400};
    torch::Tensor input_ids = torch::tensor(token_ids, torch::kInt64).unsqueeze(0).to(device);
    print_tensor_info("è¾“å…¥Token IDs", input_ids);
    
    // Embedding -> Attention
    {
        torch::NoGradGuard no_grad;
        attention->clear_cache();
        
        torch::Tensor embeddings = embedding->forward(input_ids);
        print_tensor_info("Embeddingè¾“å‡º", embeddings);
        
        torch::Tensor attn_output = attention->forward(embeddings, false);
        print_tensor_info("Attentionæœ€ç»ˆè¾“å‡º", attn_output);
        
        std::cout << "âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ" << std::endl;
    }

    std::cout << "\nğŸ‰ Qwen Attentionå±‚æµ‹è¯•å®Œæˆ" << std::endl;
    return 0;
}
